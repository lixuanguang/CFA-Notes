\subsubsection{Quantitative Methods: Multiple Regression}

ANOVA Table\\
\begin{tabularx}{\textwidth}{X|X|X|p{12em}|X}
\hline
\rowcolor{gray!30}
Source & Sum of Squares & $df$ & Mean Square & $F$-Statistics \\
\hline
Regression & $SSR$ & $k$ & $MSR = \frac{SSR}{k}$ & $F = \frac{MSR}{MSE}$ \\
Error & $SSE$ & $n-k-1$ & $MSE = \frac{SSE}{n-k-1} = SE^2$ & \\
Total & $SST$ & $n-1$ & &\\
\hline
\end{tabularx}

\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Logistic Regression: uses $log$-odds.
\begin{align}
\ln \frac{P}{1-P} &= \beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n + \epsilon \nonumber \\
P &= \frac{1}{1 + \exp[-(\beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n)]} \nonumber
\end{align}
\item $t$-value for Regression Coefficient: reject $H_0: \beta_i = 0$ if $t$-statistic < $t$-critical for two-tailed test, where $\beta_i$ is coefficient value, and $SE_i$ is standard error of coefficient.
\begin{equation}
t_{\beta_i} = \frac{\beta_i}{SE_i} \nonumber
\end{equation}
\item R-Squared: 
\begin{equation}
R^2 = \frac{SSR}{SST} \nonumber
\end{equation}
\item Adjusted R-Squared:
\begin{equation}
\overline{R}^2 = 1 - \left( \frac{n-1}{n-k-1} \right)(1-R^2) = 1 - \frac{SSE / (n-k-1)}{SST/(n-1)} \nonumber
\end{equation}
\item Standard Error of Estimate ($SEE$): low $SEE$ implies high $R^2$.
\begin{equation}
SEE = \sqrt{\frac{SSE}{n-k-1}} \nonumber
\end{equation}
\item Restricted vs Unrestricted Model: If $F$-statistic $>$ $F$-value, unrestricted model is not significant.\\
Let $q$ be number of independent variable difference between $R, U$ model.\\
The test is one-tailed, right side, on $\alpha \%$.
\begin{equation}
F = \frac{(SSE_R - SSE_U)/q}{SSE_U/(n-k-1)}, \ \ \ df = (q, n-k-1) \nonumber
\end{equation}
\item Akaike's Information Criterion: lower means better prediction.
\begin{equation}
AIC = n \ln \left( \frac{SSE}{n} \right) + 2 (k+1) \nonumber
\end{equation}
\item Bayesian Information Criterion: lower means better fit.
\begin{equation}
BIC = n \ln \left( \frac{SSE}{n} \right) + \ln (n) (k+1) \nonumber
\end{equation}
\item Test for Conditional Heteroskedasticity: Breusch-Pagan (BP) Test. One-tailed test\\
If test statistic $>$ test critical, reject $H_0$. Hence evidence of heteroskedasticity.\\
Let $n$ be number of observations, $R^2$ be regression from BP test.
\begin{equation}
\text{BP Test Statistic} = \chi^2_{k} = nR^2, \ \ \ df = k \nonumber
\end{equation}
\item Test of Multi-Collinearity: Variance Inflation Factor (VIF).
\begin{align}
VIF_j = \frac{1}{1 - R^2_j} \nonumber
\end{align}
$VIF_j > 5$ warrants investigation.\\
$VIF_j > 10$ indicates serious multi-collinearity.
\item Test of Leverage: to detect points of extreme observations of independent variable.\\
If $h_{ii} > 3 (\frac{k+1}{n})$, it is considered potentially influential. $k$ is $\#$ independent variable.
\item Test of Outlier: Studentised Residuals. Extreme observations of dependent variable.
\begin{equation}
t_{i^*} = \frac{e^{*}_i}{s_{e^*}} = \frac{e_i}{\sqrt{MSE_{(i)} (1-h_{ii})}} \nonumber
\end{equation}
where $e^{*}_i$ is residual with $i$th observation deleted, $s_{e^*}$ is s.d. of all residuals, $k$ is number of independent variables, $MSE_{(i)}$ is MSE of regression model that deletes $i$th observation, $h_{ii}$ is $i$th observation leverage.\\
If $\abs{t_{i^*}} > 3$, flag observation as outlier.\\
If $\abs{t_{i^*}} >$ $t$ critical value with $df=n-k-2$, potential influential
\item Test of Influential Data Points: Cook's Distance. Points exclusion causes substantial changes in regression.\\
\begin{equation}
D_i = \frac{e_i^2}{(k+1)MSE} \left[ \frac{h_{ii}}{(1-h_{ii})^2} \right] \nonumber
\end{equation}
where $e_i$ is residual for observation $i$.\\
Cook's distance is distributed as an $F$-distribution with $df = (k+1, n-k-1)$.\\
If $D_i \geq 0.5$, observation may be influential and merits further investigation.\\
If $D_i \geq 1$ or $D_i \geq \sqrt{k/n}$, observation is highly likely to be an influential datapoint.
\end{enumerate}


\subsubsection{Quantitative Methods: Time Series Analysis}

\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Test for Serial Correlation: Breusch-Godfrey (BG) Test.\\
If test statistic $>$ test critical, reject $H_0$. Hence evidence of serial correlation.\\
$F$-test $df = (p, n-p-k-1)$ where $p$ is $\#$ lag, $n$ is $\#$ observation, $k$ is $\#$ independent variables.
\item Test for Serial Correlation: Durbin-Watson (DW) Test.\\
If test value $< d_L$, reject $H_0$. Hence evidence of positive serial correlation.\\
If test value $> d_U$, do not reject $H_0$.
\item Standard Error of Correlations: let $T$ be number of observations in regression.
\begin{equation}
SE = \frac{1}{\sqrt{T}} \nonumber
\end{equation}
\item Test of Autocorrelation of Residual: if $t$-statistic $>$ $\abs{t \text{ Critical}}$, residuals are serially correlated.
\begin{equation}
t = \frac{\rho_{\epsilon, k}}{SE}, \ \ \ df = T - k \nonumber
\end{equation}
where $K$ is number of parameters.
\item Test of Unit Root: Dickey-Fuller. $H_0: g_1 = b_1 - 1= 0$, $H_{\alpha}: g_1 < 0$.\\
If $t$-statistic $>$ $\abs{t \text{ Critical}}$, series does not have unit root.
\item Test of ARCH: regress squared residuals on lagged squared residuals.\\
If coefficient of lagged residual is significantly different from $0$, the time series is ARCH($1$).\\
In this case, model using generalised least squares model.
\item Two Time Series Co-Integration Test: with Engle-Granger (DF-EG) test.\\
Used when both time series have unit root, hence to check for co-integration.\\
$H_0: g = \epsilon_t - \epsilon_{t-1} = 0$, $H_{\alpha} = 1$. If $H_0$ rejected, $\epsilon_t$ is covariance-stationary. Both are co-integrated.
\item Mean-Reversion Level: given time series $X_t = b_0 + b_1 X_{t-1}$, mean reverting level is
\begin{equation}
X_t = \frac{b_0}{1 - b_1} \nonumber
\end{equation}
\end{enumerate}

\subsubsection{Quantitative Methods: Big Data and Machine Learning}

For text processing:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Term Frequency (TF): prune very low and very high TF words to remove noise.
\begin{equation}
TF = \frac{\# \text{ tokens occur}}{\# \text{total tokens}} \nonumber
\end{equation}
\item Document Frequency (DF):
\begin{equation}
DF = \frac{\# \text{ documents containing token}}{\# \text{ total document number}} \nonumber
\end{equation}
\item Inverse Document Frequency (IDF): measures how unique a term is
\begin{equation}
IDF = \log(1/DF) \nonumber
\end{equation}
\item $TF$-$IDF$ higher value indicates more important word. Metric calculated at sentence level.\\
Varies by number of documents in dataset, hence model performance can vary when applied to dataset with just a few documents.
\begin{equation}
TF\text{-}IDF = TF \times IDF \nonumber
\end{equation}
\end{enumerate}

For model performance:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Precision: high precision valued when cost of Type I error is large.
\begin{equation}
P = \frac{TP}{TP + FP} = \frac{\text{Correctly predicted positive}}{\text{All predicted positive}} \nonumber
\end{equation}
\item Recall: high recall valued when cost of Type II error is large.
\begin{equation}
P = \frac{TP}{TP + FN} = \frac{\text{Correctly predicted positive}}{\text{All actual positive}} \nonumber
\end{equation}
\item Accuracy: proportion of correct forecasts out of total number of forecasts.
\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN} \nonumber
\end{equation}
\item $F1$ Score: harmonic mean of precision and recall. Used when there is unequal class distribution in data.
\begin{equation}
F1 \text{ Score} = \frac{2 \times P \times R}{P + R} \nonumber
\end{equation}
\item Receiver Operating Characteristic (ROC): more convex is better performance.\\
Area Under Curve (AUC): $\rightarrow 1$ is perfect prediction, $\rightarrow 0.5$ is close to random guess.
\end{enumerate}