\subsection{Quantitative Methods}

\subsubsection{Multiple Regression}

\begin{method} \hlt{Logistic Regression: Changing One Variable, Keeping Others Constant}\\
Compute the dependent variable with original independent variable values first.\\
Compute the dependent variable with one of the independent variables increased by one unit.\\
The impact on dependent variable is the difference in values of the probabilities.\\
There are no shortcuts. To do the computations.
\end{method}

\begin{method} \hlt{Fit of Logistic Regression}\\
Model has better fit if it has higher (less negative) log-likelihood value.
\end{method}

\begin{method} \hlt{Test of Significance of Each Independent Variable}\\
If question only gives coefficient value $\beta_i$ and respective standard error $SE_i$, to compute $t$-values. If $t$-value for variable $i$ $>$ $t$-statistic at $\alpha \%$, do not reject null hypothesis that variable $i$ coefficient is zero.
\end{method}

\begin{method} \hlt{Computing Root-Mean-Square Errors}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Error is difference between actual and forecast values.
\item Square, then sum the errors.
\item Divide by number of forecasts, then take the square root.
\end{enumerate}
\end{method}

ANOVA Table\\
\begin{tabularx}{\textwidth}{X|X|X|p{12em}|X}
\hline
\rowcolor{gray!30}
Source & Sum of Squares & $df$ & Mean Square & $F$-Statistics \\
\hline
Regression & $SSR$ & $k$ & $MSR = \frac{SSR}{k}$ & $F = \frac{MSR}{MSE}$ \\
Error & $SSE$ & $n-k-1$ & $MSE = \frac{SSE}{n-k-1} = SE^2$ & \\
Total & $SST$ & $n-1$ & &\\
\hline
\end{tabularx}

\begin{definition} \hlt{$R^2$ and Adjusted $R^2$}\\
Note $R^2$ explains the $\%$ of variation in the dependent variable of a model.\\
If Adjusted $R^2$ decreases with additional of new independent variable, the new variable does not improve the explanatory power of the new model. Only increase if new variable $t$-statistics $> \abs{1.0}$.
\end{definition}

\begin{method} \hlt{Assessing Model Best Fit}\\
Lower BIC means better fit.\\
Lower AIC means better forecast.
\end{method}

\begin{method} \hlt{Assessing Model Significance for Restricted vs Unrestricted Model}\\
Compute the joint $F$-statistic at the significance level.\\
Reject if $F$-value $> F$-statistic. Unrestricted model is not significant.
\end{method}

\begin{method} \hlt{Heteroskedasticity and Homoskedasticity}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Homoskedasticity: variance of residuals is constant for all observations.\\
Checked with scatterplot of regression residuals against predicted values.
\item Unconditional Heteroskedasticity: error variance uncorrelated with independent variables.\\
Violate equal variance, but no major problems for regression.
\item Conditional Heteroskedasticity: error variance conditional on independent variables.\\
Significant problems for statistical inference. $MSE$ is biased, $F$-test for model is unreliable.
\end{enumerate}
\end{method}

\begin{method} \hlt{Serial Correlation}\\
Results in invalid regression coefficients.\\
Coefficient $SE$ are deflated, $t$-statistics are inflated.
\end{method}

\begin{method} \hlt{Multi-Collinearity}\\
Present if $R^2$ is high, $F$-statistic is significant, but coefficient are not significant.\\
May be addressed by excluding one or more regression variables, using different proxy for one of the variables, and increasing the sample size.
\end{method}

\subsubsection{Time Series Regression}

\begin{method} \hlt{Requirements for Covariance Stationary Time Series}\\
Constant mean, variance, and covariance (of time series with lagged of itself) in all periods.\\
If $\beta_1 < 1$, series is mean reverting and is covariance stationary.\\
If time series is not covariance stationary (i.e. no finite mean reverting level), use first-differencing.
\end{method}

\begin{method} \hlt{Steps for Time Series Testing}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Test for serial correlation with Durbin-Watson (DW) or Breusch-Godfrey (BG).
\item If time series has serial correlation, test residuals for each lag with correlation. Continue increasing lag until errors from final AR($p$) model are not serially correlated.
\item When serial correlation is eliminated, test for seasonality.
\item When seasonality is eliminated, test for ARCH behaviour.
\end{enumerate}
\end{method}

\begin{method} \hlt{Use of Durbin-Watson (DW) to Test Autocorrelations}\\
DW statistic cannot be used for regression that has a lagged value of dependent variable.\\
To test for autocorrelations, examine the $t$-statistic of autocorrelations.
\end{method}
