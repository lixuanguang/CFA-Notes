\subsection{Quantitative Methods}

\subsubsection{Multiple Regression}

Reject $H_0$ if $p$-value $<$ significance level, or if $F$- or $t$- etc. statistics $>$ critical value.

\begin{method} \hlt{Logistic Regression: Changing One Variable, Keeping Others Constant}\\
Compute the dependent variable with original independent variable values first.\\
Compute the dependent variable with one of the independent variables increased by one unit.\\
The impact on dependent variable is the difference in values of the probabilities.\\
There are no shortcuts. To do the computations.
\end{method}

\begin{method} \hlt{Fit of Logistic Regression}\\
Model has better fit if it has higher (less negative) log-likelihood value.
\end{method}

\begin{method} \hlt{Test of Significance of Each Independent Variable}\\
If question only gives coefficient value $\beta_i$ and respective standard error $SE_i$, to compute $t$-values. If $t$-value for variable $i$ $>$ $t$-statistic at $\alpha \%$, do not reject null hypothesis that variable $i$ coefficient is zero.
\end{method}

\begin{method} \hlt{Computing Root-Mean-Square Errors}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Error is difference between actual and forecast values.
\item Square, then sum the errors.
\item Divide by number of forecasts, then take the square root.
\end{enumerate}
The smaller the RMSE, the more accurate the model.
\end{method}

ANOVA Table\\
\begin{tabularx}{\textwidth}{X|X|X|p{12em}|X}
\hline
\rowcolor{gray!30}
Source & Sum of Squares & $df$ & Mean Square & $F$-Statistics \\
\hline
Regression & $SSR$ & $k$ & $MSR = \frac{SSR}{k}$ & $F = \frac{MSR}{MSE}$ \\
Error & $SSE$ & $n-k-1$ & $MSE = \frac{SSE}{n-k-1} = SE^2$ & \\
Total & $SST$ & $n-1$ & &\\
\hline
\end{tabularx}

\begin{definition} \hlt{$R^2$ and Adjusted $R^2$}\\
Note $R^2$ explains the $\%$ of variation in the dependent variable of a model.\\
If Adjusted $R^2$ decreases with additional of new independent variable, the new variable does not improve the explanatory power of the new model. Only increase if new variable $t$-statistics $> \abs{1.0}$.
\end{definition}

\begin{method} \hlt{Assessing Model Best Fit}\\
Lower BIC means better fit.\\
Lower AIC means better forecast.
\end{method}

\begin{method} \hlt{Assessing Model Significance for Restricted vs Unrestricted Model}\\
Compute the joint $F$-statistic at the significance level.\\
Reject if $F$-value $> F$-statistic. Unrestricted model is not significant.
\end{method}

\begin{method} \hlt{Heteroskedasticity and Homoskedasticity}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Homoskedasticity: variance of residuals is constant for all observations.\\
Checked with scatterplot of regression residuals against predicted values.
\item Unconditional Heteroskedasticity: error variance uncorrelated with independent variables.\\
Violate equal variance, but no major problems for regression.
\item Conditional Heteroskedasticity: error variance conditional on independent variables.\\
Significant problems for statistical inference. $MSE$ is biased, $F$-test for model is unreliable.
\end{enumerate}
\end{method}

\begin{method} \hlt{Serial Correlation}\\
Results in invalid regression coefficients.\\
Coefficient $SE$ are deflated, $t$-statistics are inflated.
\end{method}

\begin{method} \hlt{Multi-Collinearity}\\
Present if $R^2$ is high, $F$-statistic is significant, but coefficient are not significant.\\
May be addressed by excluding one or more regression variables, using different proxy for one of the variables, and increasing the sample size.
\end{method}

\subsubsection{Time Series Regression}

\begin{method} \hlt{Requirements for Covariance Stationary Time Series}\\
Constant mean, variance, and covariance (of time series with lagged of itself) in all periods.\\
If $\beta_1 < 1$, series is mean reverting and is covariance stationary.\\
If time series is not covariance stationary (i.e. no finite mean reverting level), use first-differencing.
\end{method}

\begin{method} \hlt{Steps for Time Series Testing}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Test for serial correlation with Durbin-Watson (DW) or Breusch-Godfrey (BG).
\item If time series has serial correlation, test residuals for each lag with correlation. Continue increasing lag until errors from final AR($p$) model are not serially correlated.
\item When serial correlation is eliminated, test for seasonality.
\item When seasonality is eliminated, test for ARCH behaviour.
\end{enumerate}
\end{method}

\begin{method} \hlt{Use of Durbin-Watson (DW) to Test Autocorrelations}\\
DW statistic cannot be used for regression that has a lagged value of dependent variable.\\
To test for autocorrelations, examine the $t$-statistic of autocorrelations.
\end{method}

\begin{method} \hlt{Random Walk}\\
Has undefined mean-reverting level, hence is not covariance stationary.\\
Has unit root of $\beta_1 = 1$, hence regression cannot be used.\\
Tested with Dickey-Fuller Unit Root Test.\\
If unit root exists, difference the time series to transform into covariance-stationary time series.
\end{method}

\begin{method} \hlt{Two Time Series}\\
Both time series must be tested for unit root. If neither series has unit root, linear regression may be used.
\end{method}

\subsubsection{Big Data and Machine Learning}

\begin{method} \hlt{Building Structured Data-Based ML Models}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Conceptualisation of modelling task
\item Data collection
\item Data preparation and wrangling:\\
\text{Winsorisation}: replaces extreme values and outliers with min and max of non-outlier points.
\item Data exploration
\item Model training
\end{enumerate}
\end{method}

\begin{method} \hlt{Building Text-Based ML Models}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Text problem formulation
\item Data (text) curation
\item Text preparation and wrangling
\begin{enumerate}[label=\arabic*.]
\setlength{\itemsep}{0pt}
\item Preprocessing: Remove extra white space, all html tags.\\
Replace punctuations with annotations to preserve grammatical meaning in text.\\
\hlt{Lemmatisation}: convert inflected forms of word into morphological root.
\item Tokenisation: Splits given text into separate tokens.
\item Normalisation: to create the bag-of-words (BOW)
\item Pruning: remove tokens with very low and very high term-frequency (TF) values.
\end{enumerate}
\item Text exploration
\item Model training
\end{enumerate}
\end{method}

\begin{method} \hlt{Purpose of Feature Selection and Feature Engineering}\\
Feature selection used to minimise model overfitting.\\
Feature engineering used to prevent under-fitting in training of model.
\end{method}

\begin{definition} \hlt{Slight Regularisation}\\
Prediction error on training dataset is small, prediction error on cross-validation data set is significantly larger. Difference in error is variance, due to too many features and model complexity. Results in model overfitting.
\end{definition}

\begin{method} \hlt{Model Selection}\\
Governed by usage of supervised or unsupervised learning, type of data, size of data.
\end{method}

\begin{method} \hlt{Model Evaluation}\\
Measures goodness of fit for validation of the model.\\
Includes error analysis, receiver operating characteristics (ROC) plots, RMSE calculations.
\end{method}
